{"id": "exp-001-rpbridge-compression", "title": "Feature Selection for RPBridge Compression", "assumption": "All 26 steps in RPBridge are necessary for accurate hand evaluation", "hypothesis": "A subset of k ≤ 8 features from RPBridge can achieve ≥95% correlation with full 26-step evaluation", "evaluationPlan": "Generate 10K random bridge hands, compute all 26 RPBridge features, use exhaustive search for k≤6 and greedy forward selection for k≤8, test linear/polynomial combinations and decision trees", "implications": "If successful, enables practical hand evaluation with <8 steps that humans can actually use", "relatedWork": "Feature selection in ML, bridge hand evaluation literature, RPBridge methodology", "milestones": ["Implement RPBridge feature extraction", "Generate hand dataset", "Run feature selection algorithms", "Evaluate correlation metrics"], "successCriteria": "Pearson correlation r ≥ 0.95 with original RPBridge scores using k ≤ 8 features", "priority": "high", "status": "proposed", "notes": "Core experiment directly testing main hypothesis. Low technical risk.", "createdDate": "2025-08-27T19:27:00.000Z"}
{"id": "exp-002-ml-vs-rules", "title": "ML Model vs Rule-Based Evaluation", "assumption": "Hand-crafted rules are optimal for bridge hand evaluation", "hypothesis": "Machine learning models trained on expert evaluations can outperform rule-based evaluators by >5%", "evaluationPlan": "Collect expert bridge player evaluations (1-10 scale) for 1000+ hands, train Random Forest/Gradient Boosting/Neural Network models, compare against RPBridge/Milton Work/point counting", "implications": "Demonstrates potential for data-driven approaches to exceed human-engineered rules", "relatedWork": "ML applications in games, expert system evaluation, bridge AI literature", "milestones": ["Collect expert hand evaluations", "Extract feature representations", "Train ML models", "Benchmark against rule-based methods"], "successCriteria": "Classification accuracy >90% on expert labels, >5% improvement over best rule-based method", "priority": "high", "status": "proposed", "notes": "Essential for establishing ML viability. Requires significant data collection effort.", "createdDate": "2025-08-27T19:27:00.000Z"}
{"id": "exp-003-contextual-evaluation", "title": "Contextual Hand Evaluation with Auction Data", "assumption": "Hand evaluation can be done independently of bidding context", "hypothesis": "Incorporating auction context improves hand evaluation accuracy by >15% for predicting contract outcomes", "evaluationPlan": "Use tournament data with bidding sequences and final contracts, compare hand evaluation with/without auction information, measure correlation with actual contract success", "implications": "Could revolutionize bridge hand evaluation by making it context-dependent rather than absolute", "relatedWork": "Contextual evaluation in games, bridge bidding systems, tournament analysis", "milestones": ["Acquire tournament datasets", "Parse bidding sequences", "Implement contextual features", "Evaluate contract prediction"], "successCriteria": ">15% improvement in predicting contract outcomes when using auction context", "priority": "medium", "status": "proposed", "notes": "High impact if successful but requires complex tournament data parsing.", "createdDate": "2025-08-27T19:27:00.000Z"}
{"id": "exp-004-feature-ablation", "title": "Feature Category Ablation Study", "assumption": "All feature categories contribute equally to hand evaluation accuracy", "hypothesis": "Shape features contribute most significantly (10-15% performance drop when removed), followed by suit quality and honor concentration", "evaluationPlan": "Systematically remove feature categories (shape, honor concentration, suit quality) from best-performing model, measure performance impact", "implications": "Identifies most critical aspects of hand evaluation for focused improvement", "relatedWork": "Ablation studies in ML, bridge hand evaluation principles", "milestones": ["Implement feature categorization", "Run ablation experiments", "Quantify performance impacts"], "successCriteria": "Clear ranking of feature importance with quantified performance impacts", "priority": "medium", "status": "proposed", "notes": "Supports understanding of what makes hand evaluation effective.", "createdDate": "2025-08-27T19:27:00.000Z"}
{"id": "exp-005-cross-validation", "title": "Robust Cross-Validation Framework", "assumption": "Random train/test splits adequately validate model performance", "hypothesis": "Stratified CV by hand strength and time-based splits for tournament data provide more realistic performance estimates", "evaluationPlan": "Implement stratified 5-fold CV by hand strength quartiles, time-based splits for tournament data, bootstrap confidence intervals", "implications": "Ensures experimental results are robust and generalizable", "relatedWork": "Cross-validation methodologies, time series validation, statistical significance testing", "milestones": ["Implement stratified CV", "Set up time-based splits", "Run bootstrap analysis", "Compare validation strategies"], "successCriteria": "Validation framework that provides stable, realistic performance estimates with proper confidence intervals", "priority": "low", "status": "proposed", "notes": "Essential for rigorous experimental validation but not core research contribution.", "createdDate": "2025-08-27T19:27:00.000Z"}